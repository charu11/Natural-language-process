{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "natural_language_process.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPm58o/IBw/wr4B83tVZI0x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charu11/Natural-language-process/blob/nlp/natural_language_process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhnYLDN27b6N",
        "colab_type": "code",
        "outputId": "38d8a914-4fb0-4ee0-8eb5-3578f1afb3d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_PwOvVo7-Ip",
        "colab_type": "code",
        "outputId": "87df7921-2990-439a-910b-666a5fa372fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM0OV3yW8PE5",
        "colab_type": "code",
        "outputId": "037132a0-89c8-4f59-9d05-cc3679566267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "ex_txt = 'Hello Mr. Smith, how are you doing today? weather is great, python is great, you cant swim, i love arsenal'\n",
        "\n",
        "print(sent_tokenize(ex_txt))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello Mr. Smith, how are you doing today?', 'weather is great, python is great, you cant swim, i love arsenal']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpVb0dL0_378",
        "colab_type": "code",
        "outputId": "35199a81-4e99-4806-af89-64df56c20b26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(word_tokenize(ex_txt))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'weather', 'is', 'great', ',', 'python', 'is', 'great', ',', 'you', 'cant', 'swim', ',', 'i', 'love', 'arsenal']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHbI0yyeBakR",
        "colab_type": "code",
        "outputId": "b413ff80-4fdd-4469-d6ea-450326c5878b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#......................stop words......................\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "example_sent = 'This is a sample sentence, showing off the stop words filtration.'\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_tokens = word_tokenize(example_sent)\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "  if w not in stop_words:\n",
        "    filtered_sentence.append(w)\n",
        "\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBZtmNBsDwCR",
        "colab_type": "code",
        "outputId": "1b557bf3-9710-4d04-abb6-e9ad9f8e2b26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# these are the stop words in english\n",
        "\n",
        "print( set(stopwords.words('english')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'theirs', 'i', 'again', 'myself', 'any', 'should', 'those', 'their', 'into', 'there', 'when', 'few', 'no', \"didn't\", 'a', 'other', 'ma', 'having', 'between', 'before', 'doing', \"that'll\", 'on', 'itself', 'hadn', 'with', 'how', 'won', 'if', 'nor', 'yourselves', 'whom', 'ours', 'weren', 'its', 'your', \"mightn't\", 'them', 'by', 'y', 'haven', 'of', 'each', 'for', 'out', 'his', 'only', \"don't\", \"isn't\", 'too', 'ain', \"weren't\", 'wouldn', 'this', 'both', 'because', 'isn', 'at', 'while', 'same', 'until', 'doesn', 'an', 'did', 'yours', \"haven't\", 'd', 'what', 'which', 'now', 'as', 'these', 'didn', 'once', 'who', 'to', 'yourself', 'more', 'can', \"you're\", 'he', 're', \"shouldn't\", \"mustn't\", 'our', 'will', \"hadn't\", 'be', 'themselves', 'or', 'hers', \"you've\", 'me', 'him', 'after', 'were', 'was', \"doesn't\", 'mustn', 'being', 'just', 'during', 'needn', 'had', 'does', 'do', 'further', 'm', 's', 'such', 'have', 'you', \"won't\", 'her', 'am', 'off', \"she's\", 'so', \"you'd\", 'we', 'the', 'couldn', 'has', 'don', \"you'll\", 'in', 'up', 'very', \"couldn't\", 'ourselves', \"it's\", 'all', \"wouldn't\", 'are', 'against', 'below', 'than', \"wasn't\", 'they', 'here', 'it', 't', 'from', 'she', 'shan', 'o', 'herself', 'not', 'himself', 'where', 'own', 'is', 'through', 'under', 'over', 'most', 'then', 'why', 'shouldn', \"hasn't\", 'that', 'been', 've', 'aren', 'my', 'above', 'wasn', 'll', 'and', 'hasn', \"should've\", 'mightn', \"shan't\", 'down', 'about', 'but', \"aren't\", 'some', \"needn't\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrKgciYJESUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#..................stemming the words......................\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6fn2SezE3zs",
        "colab_type": "code",
        "outputId": "0a4ba225-cb00-4b63-abcc-65d1747d3072",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
        "\n",
        "for w in example_words:\n",
        "  print(ps.stem(w))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python\n",
            "python\n",
            "python\n",
            "python\n",
            "pythonli\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TzOyga4FTQQ",
        "colab_type": "code",
        "outputId": "c7ce02bb-088f-470a-e99a-44934ad725b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "new_text = \"It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n",
        "\n",
        "words = word_tokenize(new_text)\n",
        "\n",
        "for w in words:\n",
        "  print(ps.stem(w))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It\n",
            "is\n",
            "import\n",
            "to\n",
            "by\n",
            "veri\n",
            "pythonli\n",
            "while\n",
            "you\n",
            "are\n",
            "python\n",
            "with\n",
            "python\n",
            ".\n",
            "all\n",
            "python\n",
            "have\n",
            "python\n",
            "poorli\n",
            "at\n",
            "least\n",
            "onc\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ30OmMBGaat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "POS tag list:\n",
        "\n",
        "CC\tcoordinating conjunction\n",
        "CD\tcardinal digit\n",
        "DT\tdeterminer\n",
        "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
        "FW\tforeign word\n",
        "IN\tpreposition/subordinating conjunction\n",
        "JJ\tadjective\t'big'\n",
        "JJR\tadjective, comparative\t'bigger'\n",
        "JJS\tadjective, superlative\t'biggest'\n",
        "LS\tlist marker\t1)\n",
        "MD\tmodal\tcould, will\n",
        "NN\tnoun, singular 'desk'\n",
        "NNS\tnoun plural\t'desks'\n",
        "NNP\tproper noun, singular\t'Harrison'\n",
        "NNPS\tproper noun, plural\t'Americans'\n",
        "PDT\tpredeterminer\t'all the kids'\n",
        "POS\tpossessive ending\tparent\\'s\n",
        "PRP\tpersonal pronoun\tI, he, she\n",
        "PRP$\tpossessive pronoun\tmy, his, hers\n",
        "RB\tadverb\tvery, silently,\n",
        "RBR\tadverb, comparative\tbetter\n",
        "RBS\tadverb, superlative\tbest\n",
        "RP\tparticle\tgive up\n",
        "TO\tto\tgo 'to' the store.\n",
        "UH\tinterjection\terrrrrrrrm\n",
        "VB\tverb, base form\ttake\n",
        "VBD\tverb, past tense\ttook\n",
        "VBG\tverb, gerund/present participle\ttaking\n",
        "VBN\tverb, past participle\ttaken\n",
        "VBP\tverb, sing. present, non-3d\ttake\n",
        "VBZ\tverb, 3rd person sing. present\ttakes\n",
        "WDT\twh-determiner\twhich\n",
        "WP\twh-pronoun\twho, what\n",
        "WP$\tpossessive wh-pronoun\twhose\n",
        "WRB\twh-abverb\twhere, when\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G6lbevDGmJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn8Cnd-TG-40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR8DV-iMHCMe",
        "colab_type": "code",
        "outputId": "a1af1297-e38e-4b5b-b9c7-c2470693ba74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tkinter import *\n",
        "import nltk\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "\n",
        "def process_content():\n",
        "    try:\n",
        "        for i in tokenized:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            #print(tagged)\n",
        "            chunk_gram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
        "            chunk_parser = nltk.RegexpParser(chunk_gram)\n",
        "            chunked = chunk_parser.parse(tagged)\n",
        "            chunked.draw()\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "process_content()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no display name and no $DISPLAY environment variable\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxDwRjXAPtVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#.................. name entity recognition.........................\n",
        "\n",
        "import nltk\n",
        "#nltk.download('all')\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "train_text = state_union.raw('2005-GWBush.txt')\n",
        "sample_text = state_union.raw('2006-GWBush.txt')\n",
        "\n",
        "\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "\n",
        "def process_content():\n",
        "  try:\n",
        "    for i in tokenized[:5]:\n",
        "      words = nltk.word_tokenize(i)\n",
        "      tagged = nltk.pos_tag(words)\n",
        "      named_ent = nltk.ne_chunk(tagged, binary=True)\n",
        "      named_ent.draw()\n",
        "\n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "\n",
        "process_content()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBAq5JW7PtOb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "63ab8532-007d-4d2c-e20c-19535192341b"
      },
      "source": [
        "#..................lemmatizing.............................\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(lemmatizer.lemmatize(\"cats\"))\n",
        "print(lemmatizer.lemmatize(\"cacti\"))\n",
        "print(lemmatizer.lemmatize(\"geese\"))\n",
        "print(lemmatizer.lemmatize(\"rocks\"))\n",
        "print(lemmatizer.lemmatize(\"python\"))\n",
        "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
        "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
        "print(lemmatizer.lemmatize(\"run\"))\n",
        "print(lemmatizer.lemmatize(\"run\",'v'))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat\n",
            "cactus\n",
            "goose\n",
            "rock\n",
            "python\n",
            "good\n",
            "best\n",
            "run\n",
            "run\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E8RVz2M3Tvk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f85b0f7b-6ce9-4f59-988d-65ce840468f9"
      },
      "source": [
        "#................working with copora....................\n",
        "\n",
        "import nltk\n",
        "print(nltk.__file__)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/__init__.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q81rl15L4l6U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "5d5e8a0f-6d4f-41d4-c973-edf5cfddf569"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, PunktSentenceTokenizer\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# sample_text\n",
        "\n",
        "sample = gutenberg.raw('bible-kjv.txt')\n",
        "\n",
        "tok = sent_tokenize(sample)\n",
        "\n",
        "for x in range(5):\n",
        "  print(tok[x])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[The King James Bible]\n",
            "\n",
            "The Old Testament of the King James Bible\n",
            "\n",
            "The First Book of Moses:  Called Genesis\n",
            "\n",
            "\n",
            "1:1 In the beginning God created the heaven and the earth.\n",
            "1:2 And the earth was without form, and void; and darkness was upon\n",
            "the face of the deep.\n",
            "And the Spirit of God moved upon the face of the\n",
            "waters.\n",
            "1:3 And God said, Let there be light: and there was light.\n",
            "1:4 And God saw the light, that it was good: and God divided the light\n",
            "from the darkness.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}